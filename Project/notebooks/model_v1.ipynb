{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For examasdfple, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/notebook294d5a41bc-f62f18-gpt/__results__.html\n/kaggle/input/notebook294d5a41bc-f62f18-gpt/__notebook__.ipynb\n/kaggle/input/notebook294d5a41bc-f62f18-gpt/__output__.json\n/kaggle/input/notebook294d5a41bc-f62f18-gpt/custom.css\n/kaggle/input/notebook294d5a41bc-f62f18-gpt/wandb/run-20201126_072037-2nr90d5p/run-2nr90d5p.wandb\n/kaggle/input/notebook294d5a41bc-f62f18-gpt/wandb/run-20201126_072037-2nr90d5p/logs/debug.log\n/kaggle/input/notebook294d5a41bc-f62f18-gpt/wandb/run-20201126_072037-2nr90d5p/logs/debug-internal.log\n/kaggle/input/notebook294d5a41bc-f62f18-gpt/wandb/run-20201126_072037-2nr90d5p/files/wandb-summary.json\n/kaggle/input/notebook294d5a41bc-f62f18-gpt/wandb/run-20201126_072037-2nr90d5p/files/config.yaml\n/kaggle/input/notebook294d5a41bc-f62f18-gpt/wandb/run-20201126_072037-2nr90d5p/files/output.log\n/kaggle/input/notebook294d5a41bc-f62f18-gpt/wandb/run-20201126_072037-2nr90d5p/files/requirements.txt\n/kaggle/input/notebook294d5a41bc-f62f18-gpt/wandb/run-20201126_072037-2nr90d5p/files/wandb-metadata.json\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\n/kaggle/input/roberta/roberta-baseNetMLE_1.pt\n/kaggle/input/project/gpt2NetMLE_1.pt\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!unzip /kaggle/input/jigsaw-toxic-comment-classification-challenge/*","execution_count":2,"outputs":[{"output_type":"stream","text":"Archive:  /kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\r\ncaution: filename not matched:  /kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\r\ncaution: filename not matched:  /kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\r\ncaution: filename not matched:  /kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install transformers > /dev/null","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd, numpy as np\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport torch\nimport torch.nn as nn\n# import tensorflow as tf\n# tf.summary.FileWriter('log_dir') \nfrom transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n# import wandb\n# os.environ[\"WANDB_API_KEY\"] = '3ace0586afdcb3d308f9316e385f913a9463bdaa'","execution_count":4,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ntest_labels = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')\n# subm = pd.read_csv('../input/sample_submission.csv')","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re, string\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’\\n])')\ndef clean_string(s): return re_tok.sub(' ', s)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyData(torch.utils.data.Dataset):\n    def __init__(self, data, label_cols):\n        self.data = data\n        self.label_cols = label_cols #test comment\n\n    def __getitem__(self, item):\n\n        comment = clean_string(self.data.comment_text[item])\n        toxic = self.data.toxic[item]\n        severe_toxic = self.data.severe_toxic[item]\n        obscene = self.data.obscene[item]\n        threat = self.data.threat[item]\n        insult = self.data.insult[item]\n        identity_hate = self.data.identity_hate[item]\n#         none= self.data.none[item]\n        return comment, torch.FloatTensor([toxic, severe_toxic, obscene, insult, identity_hate])\n    def __len__(self):\n        return len(self.data)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n# train['none'] = 1-train[label_cols].max(axis=1)\n# label_cols.append('none')\nCOMMENT = 'comment_text'\nlabel_cols.append(COMMENT)\ntrain[COMMENT].fillna(\"unknown\", inplace=True)\ntest[COMMENT].fillna(\"unknown\", inplace=True)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpu = 0\ndevice = torch.device(gpu if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    torch.cuda.set_device(gpu)\nprint(device)","execution_count":9,"outputs":[{"output_type":"stream","text":"cuda:0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQ_LEN=512\nBATCH_SIZE =4\nLEARNING_RATE = 1e-3\nWARMUP_STEPS = 4\nEPOCHS = 2\nmodel_name= 'roberta-base'\n# wandb.init(project=model_name)\n# config = wandb.config","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=MAX_SEQ_LEN)\nimport transformers\ntokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=MAX_SEQ_LEN)\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})","execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38539f63aa784817aa5b5577f3caf783"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea41822da93740c7b49625937231d02d"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"544761b5b0714387bf973b2eb0bfa788"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"1"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass AutoNet(nn.Module):\n  def __init__(self, seqLength=MAX_SEQ_LEN, numClasses=5, model_name=model_name, len_t = len(tokenizer)):\n    super(AutoNet, self).__init__()\n    self.model = AutoModel.from_pretrained(model_name)\n    self.model.resize_token_embeddings(len_t)\n    # for p in self.bert.parameters():\n    #   p.requires_grad=False\n    self.fc = nn.Linear(seqLength*768,numClasses)\n    # self.drop = nn.Dropout(p=0.2)\n    self.sigmoid = nn.Sigmoid()\n  def forward(self,input_ids, attention_mask):\n    x = self.model(input_ids, attention_mask=attention_mask)[0] #last_hidden_state #BSD\n\n    x = self.fc(x.view(x.shape[0],-1))\n    # x = self.drop(x)\n    # x = F.softmax(x, dim=1)\n    x = self.sigmoid(x)\n    return x","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = MyData(train, label_cols)\ntrain_set, val_set = torch.utils.data.random_split(train_set, [int(0.8*len(train_set)), len(train_set)-int(0.8*len(train_set))] )\ntrain_set = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\nval_set = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = AutoNet()\nmodel = model.to(device)\nmodel.train()\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps = -1)\nloss_criteria = nn.MSELoss()#nn.BCELoss()\nloss_criteria = loss_criteria.to(device)\nmodel.load_state_dict(torch.load(f'/kaggle/input/notebook294d5a41bc-f62f18-gpt/roberta-baseNetMLE_2.pt'))","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel.train()\nfrom tqdm import tqdm\nfor epoch in tqdm(range(1)):\n  count = 0\n  total_loss =0 \n  model.train()\n  for i,data in enumerate(train_set):\n    optimizer.zero_grad()\n    enc = tokenizer.batch_encode_plus(list(data[0]), pad_to_max_length=True, max_length=MAX_SEQ_LEN, return_tensors='pt')\n    input_ids = enc['input_ids'].to(device)\n    attention_mask = enc['attention_mask'].to(device)\n    labels = torch.tensor(data[1]).to(device)\n    out = model(input_ids=input_ids, attention_mask=attention_mask)\n    # loss, logits = out[:2]\n    # print(out.shape, logits.shape, labels.shape)\n    loss = loss_criteria(out, labels)\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n    total_loss += loss.detach().data\n    if (i+1)%1024==0:\n      print(f\"Epoch: {epoch}, batch: {i+1}, loss: {total_loss/(1024*BATCH_SIZE)}\")\n#       wandb.log({\"train_loss_per_batch\": loss})\n      total_loss = 0\n  torch.save(model.state_dict(), f'{model_name}NetMLE_2.pt')\n\n  model.eval()\n  all_pred = []\n  all_gold = []\n  for i,data in enumerate(val_set):\n    enc = tokenizer.batch_encode_plus(list(data[0]), pad_to_max_length=True, max_length=MAX_SEQ_LEN, return_tensors='pt')\n    input_ids = enc['input_ids'].to(device)\n    attention_mask = enc['attention_mask'].to(device)\n    labels = torch.tensor(data[1]).to(device)\n    out = model(input_ids=input_ids, attention_mask=attention_mask)\n    all_pred.extend(1*(out>0.8).clone().detach().cpu().numpy())\n    all_gold.extend((labels.type(torch.LongTensor).detach().cpu().numpy()))\n    \n  count=0\n  for i in range(len(all_gold)):\n    if (all_gold[i]==all_pred[i]).all():\n      count+=1\n#   wandb.log({\"val_acc\": count/len(all_gold)})\n  print(\"Validation accuracy:\", count/len(all_gold))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.load_state_dict(torch.load(f'/kaggle/input/bertnet/BertNetCE_2.pt'))\nmodel.eval()\nall_pred = []\nall_gold = []\nfor i,data in enumerate(val_set):\n#     optimizer.zero_grad()\n    enc = tokenizer.batch_encode_plus(list(data[0]), pad_to_max_length=True, max_length=MAX_SEQ_LEN, return_tensors='pt')\n    input_ids = enc['input_ids'].to(device)\n    attention_mask = enc['attention_mask'].to(device)\n    labels = torch.tensor(data[1]).to(device)\n    out = model(input_ids=input_ids, attention_mask=attention_mask)\n\n    all_pred.extend(1*(out>0.8).clone().detach().cpu().numpy())\n    all_gold.extend((labels.type(torch.LongTensor).detach().cpu().numpy()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(all_gold, all_pred)","execution_count":17,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'all_gold' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-cc0c44bbe411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_gold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'all_gold' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_cols","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"['toxic',\n 'severe_toxic',\n 'obscene',\n 'threat',\n 'insult',\n 'identity_hate',\n 'comment_text']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label_cols.remove('comment_text')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_labels = test_labels[label_cols]+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels = test_labels[test_labels.toxic!=-1]\ntest_labels.sample(10)\n# a = test[:30]\n# print(a)\n# a.to_csv(r'sample_debug.csv', index = False)","execution_count":24,"outputs":[{"output_type":"stream","text":"                  id                                       comment_text\n0   00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n1   0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n2   00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n3   00017563c3f7919a  :If you have a look back at the source, the in...\n4   00017695ad8997eb          I don't anonymously edit articles at all.\n5   0001ea8717f6de06  Thank you for understanding. I think very high...\n6   00024115d4cbde0f  Please do not add nonsense to Wikipedia. Such ...\n7   000247e83dcc1211                   :Dear god this site is horrible.\n8   00025358d4737918  \" \\n Only a fool can believe in such numbers. ...\n9   00026d1092fe71cc  == Double Redirects == \\n\\n When fixing double...\n10  0002eadc3b301559  I think its crap that the link to roggenbier i...\n11  0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...\n12  0003806b11932181  , 25 February 2010 (UTC) \\n\\n :::Looking it ov...\n13  0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...\n14  00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...\n15  000634272d0d44eb  ==Current Position== \\n Anyone have confirmati...\n16  000663aff0fffc80                           this other one from 1897\n17  000689dd34e20979  == Reason for banning throwing == \\n\\n This ar...\n18  000834769115370c  :: Wallamoose was changing the cited material ...\n19  000844b52dee5f3f             |blocked]] from editing Wikipedia.   |\n20  00084da5d4ead7aa  ==Indefinitely blocked== \\n I have indefinitel...\n21  00091c35fa9d0465  == Arabs are committing genocide in Iraq, but ...\n22  000968ce11f5ee34  Please stop. If you continue to vandalize Wiki...\n23  0009734200a85047  == Energy  == \\n\\n I have edited the introduct...\n24  00097b6214686db5  :yeah, thanks for reviving the tradition of pi...\n25  0009aef4bd9e1697  MLM Software,NBFC software,Non Banking Financi...\n26  000a02d807ae0254  @RedSlash, cut it short. If you have sources s...\n27  000a6c6d4e89b9bc  ==================== \\n Deception is the way o...\n28  000bafe2080bba82  . \\n\\n           Jews are not a race because y...\n29  000bf0a9894b2807  :::If Ollie or others think that one list of t...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest_set = test.merge(test_labels, left_index=True, right_index=True)\ntest_set = test_set[[\"id_x\", \"comment_text\", \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]\ntest_set = test_set.reset_index(drop=True)\ntest_set = test_set.rename(columns={\"id_x\": \"id\"})\n\n# test_set['none'] = 1-test_set[label_cols].max(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set = MyData(test_set, label_cols)\ntest_set = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.load_state_dict(torch.load(f'/kaggle/input/bertnet/BertNetCE_2.pt'))\nmodel.eval()\nall_pred = []\nall_gold = []\nfor i,data in enumerate(test_set):\n#     optimizer.zero_grad()\n    enc = tokenizer.batch_encode_plus(list(data[0]), pad_to_max_length=True, max_length=MAX_SEQ_LEN, return_tensors='pt')\n    input_ids = enc['input_ids'].to(device)\n    attention_mask = enc['attention_mask'].to(device)\n    labels = torch.tensor(data[1]).to(device)\n    out = model(input_ids=input_ids, attention_mask=attention_mask)\n\n    all_pred.extend(1*(out>0.8).clone().detach().cpu().numpy())\n    all_gold.extend((labels.type(torch.LongTensor).detach().cpu().numpy()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(all_gold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(all_gold, all_pred)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count=0\nfor i in range(len(all_gold)):\n    if (all_gold[i]==all_pred[i]).all():\n        count+=1\ncount/len(all_gold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wandb.log({\"test_acc\": count/len(all_gold) })","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}