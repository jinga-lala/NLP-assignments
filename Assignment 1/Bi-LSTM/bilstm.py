# -*- coding: utf-8 -*-
"""POS tagging by BiLSTM

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g60Oo_IMghBZxYRmUDw056n7twnpSqfK
"""

import nltk
from nltk.corpus import brown
import random 
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from argparse import ArgumentParser
ap = ArgumentParser()
ap.add_argument("--cross_validation", type=int, default=0)
av = ap.parse_args()

def importdata():
	nltk.download('brown')
	nltk.download('universal_tagset')


def getRows():

	Sentences = nltk.corpus.brown.sents(categories=nltk.corpus.brown.categories())
	return Sentences


def getTaggedWords():

	return nltk.corpus.brown.tagged_words(tagset='universal')


def getTaggedSent():
	return nltk.corpus.brown.tagged_sents(categories= nltk.corpus.brown.categories(), tagset='universal')


def Parition_in_n(input_list, n):
    random.shuffle(input_list)
    return [input_list[i::n] for i in range(n)]

importdata()

List_of_Tags = []
List_of_Words = []

indextags = {}
indexwords = {}

Tagged_words = getTaggedWords()

Tagged_Sentences = getTaggedSent()

for words in Tagged_words:
  List_of_Tags.append(words[1])
  List_of_Words.append(words[0].lower())

List_of_Words.append('st')
List_of_Words.append('endt')

List_of_Tags.append('st')
List_of_Tags.append('endt')

Set_of_Words = list(set(List_of_Words))
Set_of_Tags = list(set(List_of_Tags))

###########Smoothening Parameters###########
Lambda = 0.01
V = len(Set_of_Words)
#############################################

Dataset_Partition = Parition_in_n(list(Tagged_Sentences),5)

#########Assigning Index to Words and Tagset for Probability Matrix######
for x in range(0,len(Set_of_Tags)):
  indextags[Set_of_Tags[x]] = x

for x in range(0,len(Set_of_Words)):
  indexwords[Set_of_Words[x]] = x

import torch
import torch.nn as nn
class BiLSTMNet(nn.Module):
  def __init__(self, hidden_dim, num_words, n_layers=1, seqLength=20, numTags=14):
    super(BiLSTMNet, self).__init__()
    self.hidden_dim = hidden_dim
    self.embeddings = nn.Embedding(num_words, hidden_dim)
    self.bilstm = nn.LSTM(hidden_dim, hidden_dim, n_layers, batch_first=True, bidirectional = True)
    self.fc = nn.Linear(hidden_dim*2,numTags)
    self.dropout = nn.Dropout(0.3)
    self.softmax = nn.Softmax(dim=2)

  def forward(self,x):
    x = self.embeddings(x)
    unpacked, (h_n, c_n) = self.bilstm(x)
    out = self.fc(unpacked)
    out = self.dropout(out)
    return self.softmax(out)

class Data:
  def __init__(self, indexwords, indextags, batch_size, seq_length):
    self.indexwords = indexwords.copy()
    self.indextags = indextags
    self.batch_size = batch_size
    self.seq_length = seq_length
      
  def one_hot_word(self, word):

    return self.indexwords[word]

  def one_hot_tag(self, tag):
    x = np.zeros(len(self.indextags))
    x[self.indextags[tag]] = 1.
    return np.asarray(x)

  def preprocess(self, dataset_list):
    word_dataset = []
    tag_dataset = []
    word_dataset_batch = []
    tag_dataset_batch = []
    seq_len = []
    for i in range(len(dataset_list)):
      wd = []
      td = []
      for j in dataset_list[i]:
        word = self.one_hot_word(j[0].lower())
        tag = self.one_hot_tag(j[1])
        wd.append(word)
        td.append(tag)
      seq_len.append(len(wd))

      word_dataset.append(np.array(wd))
      tag_dataset.append(np.array(td))
    
    return word_dataset, tag_dataset

gpu = 0
device = torch.device(gpu if torch.cuda.is_available() else "cpu")
if torch.cuda.is_available():
    torch.cuda.set_device(gpu)
print(device)


for test_index in range(0,5):
  Train_set = []
  Test_set = []
  for training_index in range(0,5):

    if training_index == test_index:
      Test_set.extend(Dataset_Partition[training_index])
      continue
    Train_set.extend(Dataset_Partition[training_index])

  train_data = Data(indexwords, indextags, 32, 20)
  word_set, tag_set = train_data.preprocess(Train_set)

  test_data = Data(indexwords, indextags, 32, 20)
  test_word_set, test_tag_set = test_data.preprocess(Test_set)

  ###Model Parameters
  loss_criteria = nn.BCELoss()
  model = BiLSTMNet(256, len(indexwords))
  model = model.to(device)
  optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)
  epochs = 5
  ###Training
  a = len(word_set)
  model.train()
  loss_total = 0
  
  for _ in tqdm(range(epochs)):
    print("*"*100)
    for i in range(a):
      out = model(torch.tensor(word_set[i]).unsqueeze(0).to(device))
      loss = loss_criteria(out[0], torch.tensor(tag_set[i]).float().to(device))
      loss.backward()
      loss_total+= loss.detach().cpu()
      if i%4096==0 and i!=0:
        print(f"loss: {loss_total} for batch size:4096 cross validation: {test_index+1}" )
        
        optimizer.step()
        optimizer.zero_grad()
        loss_total = 0
    
  ###Testing
  b = len(test_word_set)
  model.eval()
  pred_all = []
  gold_all = []
  for i in range(b):
    out = model(torch.tensor(test_word_set[i]).unsqueeze(0).to(device))
    
    pred = torch.argmax(out[0], dim=1)
    gold = np.argmax(test_tag_set[i], axis=1)
   
    pred_all.extend(pred.detach().cpu().data)
    gold_all.extend(gold)
  
  print('-'*100)
  print(f"Performance of Cross validation: {test_index+1} ")
  print(f"Accuracy: {accuracy_score(gold_all, pred_all)}")
  print(classification_report(gold_all, pred_all))
  print('-'*100)
  if av.cross_validation==0:
    break
  
cm = confusion_matrix(gold_all, pred_all, list(indextags.values()))
print(cm)

sns.set(rc={'figure.figsize':(11.7,8.27)})


ax= plt.subplot()
sns.heatmap(cm, annot=False, ax = ax,cmap="YlGnBu")

ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.set_title('Confusion Matrix') 
ax.xaxis.set_ticklabels(list(indextags.keys()))
ax.yaxis.set_ticklabels(list(indextags.keys()));
plt.show()